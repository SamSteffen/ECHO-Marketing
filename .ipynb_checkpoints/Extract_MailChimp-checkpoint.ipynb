{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this script is to retrieve raw data downloaded from MailChimp as a CSV file, clean it, and merge it with similar data in preparation for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import matplotlib\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Download all the desired data from ECHO Idaho's Mailchimp account. \n",
    "# STEP 2: Once all the desired data is downloaded, navigate to the 'Downloads' folder and retrieve the desired .csv files\n",
    "# STEP 3: Store all the desired files in the following location:\n",
    "# 'C:\\\\Users\\\\ssteffen\\\\University of Idaho\\\\Storage-Boise - ECHO\\\\Staff\\\\Sam\\\\Data\\\\Spreadsheets\\\\raw_MailChimp\\\\input\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Create a variable for the root filepath to the raw data\n",
    "# /c/Users/ssteffen/University of Idaho/Storage-Boise - ECHO/Staff/Sam/Data\n",
    "data_path_root = 'C:\\\\Users\\\\ssteffen\\\\University of Idaho\\\\Storage-Boise - ECHO\\\\Staff\\\\Sam\\\\Data\\\\Projects\\\\MailChimp_formatter\\\\raw_MailChimp\\\\input\\\\'\n",
    "\n",
    "#STEP 5: get the names of all the .csv files in the directory in a list\n",
    "raw_csvs = os.listdir(data_path_root)\n",
    "raw_csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Add all of the following to a callable function that takes in raw_csvs as its parameter\n",
    "#STEP 6: Create a variable that concatenates the full file location of the .csv\n",
    "data = str(data_path_root + raw_csvs[35])\n",
    "# data\n",
    "\n",
    "#STEP 7: Read in the data as a df\n",
    "df = pd.read_csv(data, sep=\";\", skip_blank_lines=False)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 8: While the urls are still data in a single row, make them all shorter\n",
    "url_data = list(df['Email Campaign Report'])\n",
    "url_data = url_data[22:]\n",
    "\n",
    "for urls in url_data:\n",
    "    df = df.replace(urls, urls.split('?')[0])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 9: Transform the Data: split the data into multiple columns by the delimiter \",\"\n",
    "split_df = df['Email Campaign Report'].str.split(',', expand=True)\n",
    "split_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 10: Transpose the column data into headers and rows\n",
    "trans_df = split_df.transpose()\n",
    "trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 11: Make row 0 into a header column\n",
    "new_header = trans_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 12: Make the df the df minus the first row of data\n",
    "trans_df = trans_df[1:]\n",
    "\n",
    "#STEP 13: Set header row as the df header\n",
    "trans_df.columns = new_header\n",
    "\n",
    "trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 14: Merge the data in the Delivery Date/Time column into a single cell\n",
    "date_time_list = trans_df['Delivery Date/Time:']\n",
    "date_time_list = date_time_list.tolist()\n",
    "date_time = str(date_time_list[0] + \",\" + date_time_list[1] + date_time_list[2])\n",
    "trans_df.insert(2, 'Delivery Datetime', date_time)\n",
    "trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 15: Check rows 2 and 3 for other useful data\n",
    "# list(trans_df.loc[[1][0]])\n",
    "# list(trans_df.loc[[2][0]])\n",
    "# list(trans_df.loc[[3][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 16: Drop rows 2-3\n",
    "data_row = trans_df.drop([2,3])\n",
    "data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 17: Call only the desired columns in the desired order\n",
    "# list(data_row)\n",
    "\n",
    "data_row = data_row[['Title:',\n",
    "                     'Subject Line:',\n",
    "                     'Delivery Datetime',\n",
    "                     'Total Recipients:',\n",
    "                     'Successful Deliveries:',\n",
    "                     'Bounces:',\n",
    "                     'Times Forwarded:',\n",
    "                     'Forwarded Opens:',\n",
    "                     'Recipients Who Opened:',\n",
    "                     'Total Opens:',\n",
    "                     'Recipients Who Clicked:',\n",
    "                     'Total Clicks:',\n",
    "                     'Total Unsubs:',\n",
    "                     'Times Liked on Facebook:'\n",
    "                    ]]\n",
    "\n",
    "data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 18: Remove the \"\" from the data\n",
    "data_row = data_row.applymap(lambda x: x.replace('\"', ''))\n",
    "data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 19: Separate columns with 2 datasets into seperate columns and remove parenthesis from data\n",
    "data_row['Bounce Count:'] = data_row['Bounces:'].str.split(\" \", expand=True)[0]\n",
    "data_row['Bounces (%):'] = data_row['Bounces:'].str.split(\" \", expand=True)[1]\n",
    "data_row['Bounces (%):'] = data_row['Bounces (%):'].str.replace(\"(\", \"\").str.replace(\")\", \"\")\n",
    "\n",
    "data_row['Open Count:'] = data_row['Recipients Who Opened:'].str.split(\" \", expand=True)[0]\n",
    "try:\n",
    "    data_row['Opens (%):'] = data_row['Recipients Who Opened:'].str.split(\" \", expand=True)[1]\n",
    "except KeyError:\n",
    "    data_row['Opens (%):'] = data_row['Recipients Who Opened:']\n",
    "    \n",
    "try:\n",
    "    data_row['Opens (%):'] = data_row['Opens (%):'].str.replace(\"(\",\"\").str.replace(\")\",\"\")\n",
    "except KeyError:\n",
    "    data_row['Opens (%):'] = data_row['Opens (%):']\n",
    "    \n",
    "data_row['Clicks Count:'] = data_row['Recipients Who Clicked:'].str.split(\" \", expand=True)[0]\n",
    "data_row['Clicks (%):'] = data_row['Recipients Who Clicked:'].str.split(\" \", expand=True)[1]\n",
    "data_row['Clicks (%):'] = data_row['Clicks (%):'].str.replace(\"(\",\"\").str.replace(\")\",\"\")\n",
    "\n",
    "#STEP 20: Keep the split columns and remove the origin columns to avoid unncessary duplication\n",
    "data_row = data_row.drop(columns=['Bounces:', 'Recipients Who Opened:', 'Recipients Who Clicked:'])\n",
    "\n",
    "data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 20: Add a 'Series' column for 'Series' data\n",
    "data_row['Series:'] = data_row['Title:'].str.split(\"-\", expand=True)[2]\n",
    "data_row['Series:'] = data_row['Series:'].str.split(\" \", expand=True)[1]\n",
    "\n",
    "data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 21: Add a 'Type' column for email Type\n",
    "data_row['Type:'] = data_row['Title:'].str.split(\"-\", expand=True)[2]\n",
    "\n",
    "try:\n",
    "    data_row['Type:'] = data_row['Type:'].str.split(\" \", expand=True)[2]  \n",
    "except KeyError:\n",
    "    email_type = \"Daily\"\n",
    "    data_row['Type'] = email_type\n",
    "\n",
    "data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 22: Convert the datetime columns into datetime datatypes\n",
    "data_row['Delivery Datetime'] = pd.to_datetime(data_row['Delivery Datetime'])\n",
    "\n",
    "data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 23: Split the 'Delivery Datetime' column into 4 columns and convert the datatypes\n",
    "data_row['Send Date:'] = data_row['Delivery Datetime'].dt.date\n",
    "data_row['Send Time (PT):'] = data_row['Delivery Datetime'].dt.time\n",
    "data_row['Send Weekday:'] = data_row['Delivery Datetime'].dt.strftime('%A')\n",
    "\n",
    "# STEP 24: Drop the Delivery Datetime column from the data_row\n",
    "data_row.drop(columns=['Delivery Datetime'])\n",
    "\n",
    "data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 25: Write a conditional that will transform whatever series is printed to the appropriate code\n",
    "series_codes = ['BH in PC', 'OPSUD', 'CTSUDs', 'XWT', 'PBH', 'PSUD', 'COVID', 'PALTC', 'VHLC', 'Syphilis', 'PASD', 'MOUD', 'General']\n",
    "\n",
    "for value in data_row['Series:']:\n",
    "    if value == 'BH':\n",
    "        data_row.loc[data_row['Series:'] == 'BH', 'Series:'] = series_codes[0]\n",
    "    elif value == 'COVID Long Term':\n",
    "        data_row.loc[data_row['Series:'] == 'COVID Long Term', 'Series:'] = series_codes[7]\n",
    "    elif value == 'COVID-19':\n",
    "        data_row.loc[data_row['Series:'] == 'COVID-19', 'Series:'] = series_codes[6]\n",
    "    elif value == 'CT-SUD':\n",
    "        data_row.loc[data_row['Series:'] == 'CT-SUD', 'Series:'] = series_codes[2]\n",
    "    elif value == 'CTSUDs':\n",
    "        data_row.loc[data_row['Series:'] == 'CTSUDs', 'Series:'] = series_codes[2]\n",
    "    elif value == 'HCV':\n",
    "        data_row.loc[data_row['Series:'] == 'HCV', 'Series:'] = series_codes[8]\n",
    "    elif value == 'Hep-C':\n",
    "        data_row.loc[data_row['Series:'] == 'Hep-C', 'Series:'] = series_codes[8]\n",
    "    elif value == 'MOUD Consultation Hours':\n",
    "        data_row.loc[data_row['Series:'] == 'MOUD Consultation Hours', 'Series:'] = series_codes[11]\n",
    "    elif value == 'MOUD Office Hours (Solo)':\n",
    "        data_row.loc[data_row['Series:'] == 'MOUD Office Hours (Solo)', 'Series:'] = series_codes[11]\n",
    "    elif value == 'OP':\n",
    "        data_row.loc[data_row['Series:'] == 'OP', 'Series:'] = series_codes[1]\n",
    "    elif value == 'PBH':\n",
    "        data_row.loc[data_row['Series:'] == 'PBH', 'Series:'] = series_codes[4]\n",
    "    elif value == 'Pediatric Autism':\n",
    "        data_row.loc[data_row['Series:'] == 'Pediatric Autism', 'Series:'] = series_codes[10]\n",
    "    elif value == 'PedsAutism':\n",
    "        data_row.loc[data_row['Series:'] == 'PedsAutism', 'Series:'] = series_codes[10]\n",
    "    elif value == 'Podcast':\n",
    "        data_row.loc[data_row['Series:'] == 'Podcast', 'Series:'] = series_codes[10]\n",
    "    elif value == 'VH&LC':\n",
    "        data_row.loc[data_row['Series:'] == 'VH&LC', 'Series:'] = series_codes[8]\n",
    "    elif value == 'VHLC':\n",
    "        data_row.loc[data_row['Series:'] == 'VHLC', 'Series:'] = series_codes[8]\n",
    "    elif value == 'Viral Hep':\n",
    "        data_row.loc[data_row['Series:'] == 'Viral Hep', 'Series:'] = series_codes[8]\n",
    "    elif value == 'X-Waiver Training':\n",
    "        data_row.loc[data_row['Series:'] == 'X-Waiver Training', 'Series:'] = series_codes[3]\n",
    "    else:\n",
    "        data_row.loc[data_row['Series:'] == value, 'Series:'] = series_codes[12]\n",
    "\n",
    "# data_row\n",
    "\n",
    "# For reference the codes to use for each series are as follows:\n",
    "# BH in PC for 'Behavioral Health in Primary Care'\n",
    "# OPSUD for 'Opioids, Pain and Substance Use Disorders'\n",
    "# CTSUDs for 'Counseling Techniques for Substance Use Disorders'\n",
    "# XWT for 'X-Waiver Trainings'\n",
    "# PBH for 'Pediatric Behavioral Health'\n",
    "# PSUD for 'Perinatal Substance Use Disorder'\n",
    "# COVID for 'COVID-19 or Ambulatory/Acute Care COVID or COVID-19 and Other Current Infections'\n",
    "# PALTC for 'Nursing Home COVID or COVID-19 Safety for Post Acute and Long Term Care'\n",
    "# VHLC for 'Hepatitis C or Viral Hepatitis and Liver Care'\n",
    "# Syphilis for 'Syphilis'\n",
    "# PASD for 'Pediatric Autism'\n",
    "# MOUD for 'Medications for Opioid Use Disorders Consultation/Office Hours'\n",
    "# General for 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 26: Write a conditional that will transform whatever email type is printed to the appropriate code\n",
    "type_codes = ['Daily','Weekly','Newsletter','XWT Promo','Special']\n",
    "\n",
    "for value in data_row['Type:']:\n",
    "    if value == 'Day':\n",
    "        data_row.loc[data_row['Type:'] == 'Day', 'Type:'] = type_codes[0]\n",
    "    if value == 'Weekly':\n",
    "        data_row.loc[data_row['Type:'] == 'Weekly', 'Type:'] = type_codes[1]\n",
    "    if value == 'Newsletter':\n",
    "        data_row.loc[data_row['Type:'] == 'Newsletter', 'Type:'] = type_codes[2]\n",
    "    if value == 'XWT Promo':\n",
    "        data_row.loc[data_row['Type:'] == 'XWT Promo', 'Type:'] = type_codes[3]\n",
    "    else:\n",
    "        data_row.loc[data_row['Type:'] == value, 'Type:'] = type_codes[4]\n",
    "    \n",
    "data_row\n",
    "\n",
    "#For reference the codes to use for each email 'type' are as follows\n",
    "# Daily for 'Day of'\n",
    "# Weekly for 'Weekly'\n",
    "# Newsletter for 'Newsletter'\n",
    "# XWT Promo for 'XWT'\n",
    "# Special for 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_row.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 27: Reorder the columns to their desired export arrangement\n",
    "# data_row.columns\n",
    "data_row = data_row[[\n",
    "    'Send Date:',\n",
    "    'Send Weekday:',\n",
    "    'Send Time (PT):',\n",
    "    'Series:',\n",
    "    'Type:',\n",
    "    'Title:',\n",
    "    'Subject Line:',\n",
    "    'Total Recipients:',\n",
    "    'Successful Deliveries:',\n",
    "    'Bounce Count:', \n",
    "    'Times Forwarded:',\n",
    "    'Forwarded Opens:', \n",
    "    'Open Count:',\n",
    "    'Opens (%):',\n",
    "    'Total Opens:', \n",
    "    'Clicks Count:',\n",
    "    'Clicks (%):',\n",
    "    'Total Clicks:',\n",
    "    'Total Unsubs:',\n",
    "    'Times Liked on Facebook:',\n",
    "    'Bounces (%):'\n",
    "]]\n",
    "\n",
    "data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 28: Retitle the columns to desired export\n",
    "data_row = data_row.rename(columns={    \n",
    "    'Send Date:': 'Date',\n",
    "    'Send Weekday:': 'Weekday',\n",
    "    'Send Time (PT):': 'Time',\n",
    "    'Series:': 'Series',\n",
    "    'Type:': 'Type',\n",
    "    'Title:': 'Title',\n",
    "    'Subject Line:' : 'Subject',\n",
    "    'Total Recipients:': 'Total Recipients',\n",
    "    'Successful Deliveries:': 'Successful Deliveries',\n",
    "    'Bounce Count:': 'Total Bounces', \n",
    "    'Times Forwarded:': 'Times Forwarded',\n",
    "    'Forwarded Opens:': 'Forwarded Opens', \n",
    "    'Open Count:': 'Unique Opens',\n",
    "    'Opens (%):': 'Open Rate',\n",
    "    'Total Opens:': 'Total Opens', \n",
    "    'Clicks Count:': 'Unique Clicks',\n",
    "    'Clicks (%):': 'Click Rate',\n",
    "    'Total Clicks:': 'Total Clicks',\n",
    "    'Total Unsubs:': 'Unsubscribes',\n",
    "    'Times Liked on Facebook:': 'FB Likes',\n",
    "    'Bounces (%):': 'Bounce Rate'\n",
    "})\n",
    "\n",
    "data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_row.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 29: Convert the datatypes\n",
    "data_row['Date'] = pd.to_datetime(data_row['Date'], format='%Y-%m-%d')\n",
    "data_row['Total Recipients'] = pd.to_numeric(data_row['Total Recipients'])\n",
    "data_row['Successful Deliveries'] = pd.to_numeric(data_row['Successful Deliveries'])\n",
    "data_row['Total Bounces'] = pd.to_numeric(data_row['Total Bounces'])\n",
    "data_row['Times Forwarded'] = pd.to_numeric(data_row['Times Forwarded'])\n",
    "data_row['Forwarded Opens'] = pd.to_numeric(data_row['Forwarded Opens'])\n",
    "\n",
    "# data_row.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 30: Create a function out of steps 1-29 that takes a folder full of .csvs for a parameter\n",
    "\n",
    "def clean_csvs(csvs_data_file):\n",
    "    csvs_data_path_root = 'C:\\\\Users\\\\ssteffen\\\\University of Idaho\\\\Storage-Boise - ECHO\\\\Staff\\\\Sam\\\\Data\\\\Spreadsheets\\\\raw_MailChimp\\\\input\\\\'\n",
    "    raw_csvs = os.listdir(data_path_root)\n",
    "    dataset_count = len(raw_csvs)\n",
    "    \n",
    "    print(f'This program will clean {dataset_count} csv files and prepare them for analysis.')\n",
    "    \n",
    "    cleaned_data_list = []\n",
    "    \n",
    "    #create a for loop to loop through and clean all the downloaded csv files\n",
    "    i = 0\n",
    "\n",
    "    for csvs in raw_csvs:\n",
    "        data = str(data_path_root + raw_csvs[i])\n",
    "        df = pd.read_csv(data, sep=\";\", skip_blank_lines=False)\n",
    "\n",
    "        # create a df and clean the urls\n",
    "        url_data = list(df['Email Campaign Report'])\n",
    "        url_data = url_data[22:]\n",
    "\n",
    "        for urls in url_data:\n",
    "            df = df.replace(urls, urls.split('?')[0])\n",
    "\n",
    "        # split the data into columns\n",
    "        split_df = df['Email Campaign Report'].str.split(',', expand=True)\n",
    "\n",
    "        # transpose the data\n",
    "        trans_df = split_df.transpose()\n",
    "\n",
    "        # make row[0] into a header\n",
    "        new_header = trans_df.iloc[0]\n",
    "\n",
    "        # Make the df the df minus the first row of data\n",
    "        trans_df = trans_df[1:]\n",
    "\n",
    "        # Set header row as the df header\n",
    "        trans_df.columns = new_header\n",
    "\n",
    "        # Merge the data in the Delivery Date/Time column into a single cell\n",
    "        date_time_list = trans_df['Delivery Date/Time:']\n",
    "        date_time_list = date_time_list.tolist()\n",
    "        date_time = str(date_time_list[0] + \",\" + date_time_list[1] + date_time_list[2])\n",
    "        trans_df.insert(2, 'Delivery Datetime', date_time)\n",
    "\n",
    "        # Drop rows 2-3\n",
    "        data_row = trans_df.drop([2,3])\n",
    "\n",
    "        # create a new df of desired columns    \n",
    "        data_row = data_row[['Title:',\n",
    "                     'Subject Line:',\n",
    "                     'Delivery Datetime',\n",
    "                     'Total Recipients:',\n",
    "                     'Successful Deliveries:',\n",
    "                     'Bounces:',\n",
    "                     'Times Forwarded:',\n",
    "                     'Forwarded Opens:',\n",
    "                     'Recipients Who Opened:',\n",
    "                     'Total Opens:',\n",
    "                     'Recipients Who Clicked:',\n",
    "                     'Total Clicks:',\n",
    "                     'Total Unsubs:',\n",
    "                     'Times Liked on Facebook:'\n",
    "                    ]]\n",
    "\n",
    "        # remove \"\" from the data\n",
    "        data_row = data_row.applymap(lambda x: x.replace('\"', ''))\n",
    "\n",
    "        # separate values into separate columns, remove parentheses\n",
    "        data_row['Bounce Count:'] = data_row['Bounces:'].str.split(\" \", expand=True)[0]\n",
    "        data_row['Bounces (%):'] = data_row['Bounces:'].str.split(\" \", expand=True)[1]\n",
    "        data_row['Bounces (%):'] = data_row['Bounces (%):'].str.replace(\"(\", \"\").str.replace(\")\", \"\")\n",
    "        \n",
    "        data_row['Open Count:'] = data_row['Recipients Who Opened:'].str.split(\" \", expand=True)[0]\n",
    "        try:\n",
    "            data_row['Opens (%):'] = data_row['Recipients Who Opened:'].str.split(\" \", expand=True)[1]\n",
    "        except KeyError:\n",
    "            data_row['Opens (%):'] = data_row['Recipients Who Opened:']\n",
    "    \n",
    "        try:\n",
    "            data_row['Opens (%):'] = data_row['Opens (%):'].str.replace(\"(\",\"\").str.replace(\")\",\"\")\n",
    "        except KeyError:\n",
    "            data_row['Opens (%):'] = data_row['Opens (%):']\n",
    "        \n",
    "        data_row['Clicks Count:'] = data_row['Recipients Who Clicked:'].str.split(\" \", expand=True)[0]\n",
    "        data_row['Clicks (%):'] = data_row['Recipients Who Clicked:'].str.split(\" \", expand=True)[1]\n",
    "        data_row['Clicks (%):'] = data_row['Clicks (%):'].str.replace(\"(\",\"\").str.replace(\")\",\"\")\n",
    "\n",
    "        # Keep the split columns and remove the origin columns to avoid unncessary duplication\n",
    "        data_row = data_row.drop(columns=['Bounces:', 'Recipients Who Opened:', 'Recipients Who Clicked:'])\n",
    "\n",
    "        # Add 'Series' column for series data\n",
    "        data_row['Series:'] = data_row['Title:'].str.split(\"-\", expand=True)[2]\n",
    "        data_row['Series:'] = data_row['Series:'].str.split(\" \", expand=True)[1]\n",
    "\n",
    "        # Add a 'Type' column for email Type\n",
    "        data_row['Type:'] = data_row['Title:'].str.split(\"-\", expand=True)[2]\n",
    "\n",
    "        try:\n",
    "            data_row['Type:'] = data_row['Type:'].str.split(\" \", expand=True)[2]  \n",
    "\n",
    "        except KeyError:\n",
    "            email_type = \"Daily\"\n",
    "            data_row['Type'] = email_type\n",
    "\n",
    "        # convert datetime columns into appropriate datatype\n",
    "        data_row['Delivery Datetime'] = pd.to_datetime(data_row['Delivery Datetime'])\n",
    "\n",
    "        # split the datetime data into seperate columns \n",
    "        data_row['Send Date:'] = data_row['Delivery Datetime'].dt.date\n",
    "        data_row['Send Time (PT):'] = data_row['Delivery Datetime'].dt.time\n",
    "        data_row['Send Weekday:'] = data_row['Delivery Datetime'].dt.strftime('%A')\n",
    "\n",
    "        # Drop the Delivery Datetime column from the data_row\n",
    "        data_row.drop(columns=['Delivery Datetime'])\n",
    "\n",
    "        series_codes = ['BH in PC', 'OPSUD', 'CTSUDs', 'XWT', 'PBH', 'PSUD', 'COVID', 'PALTC', 'VHLC', 'Syphilis', 'PASD', 'MOUD', 'General']\n",
    "\n",
    "        # codify the series\n",
    "        for value in data_row['Series:']:\n",
    "            if value == 'BH':\n",
    "                data_row.loc[data_row['Series:'] == 'BH', 'Series:'] = series_codes[0]\n",
    "            elif value == 'COVID Long Term':\n",
    "                data_row.loc[data_row['Series:'] == 'COVID Long Term', 'Series:'] = series_codes[7]\n",
    "            elif value == 'COVID-19':\n",
    "                data_row.loc[data_row['Series:'] == 'COVID-19', 'Series:'] = series_codes[6]\n",
    "            elif value == 'CT-SUD':\n",
    "                data_row.loc[data_row['Series:'] == 'CT-SUD', 'Series:'] = series_codes[2]\n",
    "            elif value == 'CTSUDs':\n",
    "                data_row.loc[data_row['Series:'] == 'CTSUDs', 'Series:'] = series_codes[2]\n",
    "            elif value == 'HCV':\n",
    "                data_row.loc[data_row['Series:'] == 'HCV', 'Series:'] = series_codes[8]\n",
    "            elif value == 'Hep-C':\n",
    "                data_row.loc[data_row['Series:'] == 'Hep-C', 'Series:'] = series_codes[8]\n",
    "            elif value == 'MOUD Consultation Hours':\n",
    "                data_row.loc[data_row['Series:'] == 'MOUD Consultation Hours', 'Series:'] = series_codes[11]\n",
    "            elif value == 'MOUD Office Hours (Solo)':\n",
    "                data_row.loc[data_row['Series:'] == 'MOUD Office Hours (Solo)', 'Series:'] = series_codes[11]\n",
    "            elif value == 'OP':\n",
    "                data_row.loc[data_row['Series:'] == 'OP', 'Series:'] = series_codes[1]\n",
    "            elif value == 'PBH':\n",
    "                data_row.loc[data_row['Series:'] == 'PBH', 'Series:'] = series_codes[4]\n",
    "            elif value == 'Pediatric Autism':\n",
    "                data_row.loc[data_row['Series:'] == 'Pediatric Autism', 'Series:'] = series_codes[10]\n",
    "            elif value == 'PedsAutism':\n",
    "                data_row.loc[data_row['Series:'] == 'PedsAutism', 'Series:'] = series_codes[10]\n",
    "            elif value == 'Podcast':\n",
    "                data_row.loc[data_row['Series:'] == 'Podcast', 'Series:'] = series_codes[10]\n",
    "            elif value == 'VH&LC':\n",
    "                data_row.loc[data_row['Series:'] == 'VH&LC', 'Series:'] = series_codes[8]\n",
    "            elif value == 'VHLC':\n",
    "                data_row.loc[data_row['Series:'] == 'VHLC', 'Series:'] = series_codes[8]\n",
    "            elif value == 'Viral Hep':\n",
    "                data_row.loc[data_row['Series:'] == 'Viral Hep', 'Series:'] = series_codes[8]\n",
    "            elif value == 'X-Waiver Training':\n",
    "                data_row.loc[data_row['Series:'] == 'X-Waiver Training', 'Series:'] = series_codes[3]\n",
    "            else:\n",
    "                data_row.loc[data_row['Series:'] == value, 'Series:'] = series_codes[12]\n",
    "\n",
    "        # codify the email types\n",
    "        type_codes = ['Daily','Weekly','Newsletter','XWT Promo','Special']\n",
    "\n",
    "        for value in data_row['Type:']:\n",
    "            if value == 'Day':\n",
    "                data_row.loc[data_row['Type:'] == 'Day', 'Type:'] = type_codes[0]\n",
    "            if value == 'Weekly':\n",
    "                data_row.loc[data_row['Type:'] == 'Weekly', 'Type:'] = type_codes[1]\n",
    "            if value == 'Newsletter':\n",
    "                data_row.loc[data_row['Type:'] == 'Newsletter', 'Type:'] = type_codes[2]\n",
    "            if value == 'XWT Promo':\n",
    "                data_row.loc[data_row['Type:'] == 'XWT Promo', 'Type:'] = type_codes[3]\n",
    "            else:\n",
    "                data_row.loc[data_row['Type:'] == value, 'Type:'] = type_codes[4]\n",
    "\n",
    "        # reorder the columns\n",
    "        data_row = data_row[[\n",
    "            'Send Date:',\n",
    "            'Send Weekday:',\n",
    "            'Send Time (PT):',\n",
    "            'Series:',\n",
    "            'Type:',\n",
    "            'Title:',\n",
    "            'Subject Line:',\n",
    "            'Total Recipients:',\n",
    "            'Successful Deliveries:',\n",
    "            'Bounce Count:', \n",
    "            'Times Forwarded:',\n",
    "            'Forwarded Opens:', \n",
    "            'Open Count:',\n",
    "            'Opens (%):',\n",
    "            'Total Opens:', \n",
    "            'Clicks Count:',\n",
    "            'Clicks (%):',\n",
    "            'Total Clicks:',\n",
    "            'Total Unsubs:',\n",
    "            'Times Liked on Facebook:',\n",
    "            'Bounces (%):'\n",
    "        ]]\n",
    "\n",
    "        # retitle the columns\n",
    "        data_row = data_row.rename(columns={    \n",
    "            'Send Date:': 'Date',\n",
    "            'Send Weekday:': 'Weekday',\n",
    "            'Send Time (PT):': 'Time',\n",
    "            'Series:': 'Series',\n",
    "            'Type:': 'Type',\n",
    "            'Title:': 'Title',\n",
    "            'Subject Line:' : 'Subject',\n",
    "            'Total Recipients:': 'Total Recipients',\n",
    "            'Successful Deliveries:': 'Successful Deliveries',\n",
    "            'Bounce Count:': 'Total Bounces', \n",
    "            'Times Forwarded:': 'Times Forwarded',\n",
    "            'Forwarded Opens:': 'Forwarded Opens', \n",
    "            'Open Count:': 'Unique Opens',\n",
    "            'Opens (%):': 'Open Rate',\n",
    "            'Total Opens:': 'Total Opens', \n",
    "            'Clicks Count:': 'Unique Clicks',\n",
    "            'Clicks (%):': 'Click Rate',\n",
    "            'Total Clicks:': 'Total Clicks',\n",
    "            'Total Unsubs:': 'Unsubscribes',\n",
    "            'Times Liked on Facebook:': 'FB Likes',\n",
    "            'Bounces (%):': 'Bounce Rate'\n",
    "        })\n",
    "\n",
    "        # convert the data types\n",
    "        data_row['Date'] = pd.to_datetime(data_row['Date'], format='%Y-%m-%d')\n",
    "        data_row['Total Recipients'] = pd.to_numeric(data_row['Total Recipients'])\n",
    "        data_row['Successful Deliveries'] = pd.to_numeric(data_row['Successful Deliveries'])\n",
    "        data_row['Total Bounces'] = pd.to_numeric(data_row['Total Bounces'])\n",
    "        data_row['Times Forwarded'] = pd.to_numeric(data_row['Times Forwarded'])\n",
    "        data_row['Forwarded Opens'] = pd.to_numeric(data_row['Forwarded Opens'])    \n",
    "\n",
    "        #add the cleaned df to the list\n",
    "        cleaned_data_list.append(data_row)\n",
    "\n",
    "        print(f\"Dataset {i} cleaned and added to list.\")\n",
    "\n",
    "        i += 1\n",
    "            \n",
    "    #add all the cleaned dfs to a single df\n",
    "    cleaned_data = pd.concat(cleaned_data_list)\n",
    "                \n",
    "    #export the df to a .csv file\n",
    "    return cleaned_data.to_csv('C:\\\\Users\\\\ssteffen\\\\University of Idaho\\\\Storage-Boise - ECHO\\\\Staff\\\\Sam\\\\Data\\\\Projects\\\\MailChimp_formatter\\\\raw_MailChimp\\\\output\\\\Mail_Chimp_data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This program will clean 52 csv files and prepare them for analysis.\n",
      "Dataset 0 cleaned and added to list.\n",
      "Dataset 1 cleaned and added to list.\n",
      "Dataset 2 cleaned and added to list.\n",
      "Dataset 3 cleaned and added to list.\n",
      "Dataset 4 cleaned and added to list.\n",
      "Dataset 5 cleaned and added to list.\n",
      "Dataset 6 cleaned and added to list.\n",
      "Dataset 7 cleaned and added to list.\n",
      "Dataset 8 cleaned and added to list.\n",
      "Dataset 9 cleaned and added to list.\n",
      "Dataset 10 cleaned and added to list.\n",
      "Dataset 11 cleaned and added to list.\n",
      "Dataset 12 cleaned and added to list.\n",
      "Dataset 13 cleaned and added to list.\n",
      "Dataset 14 cleaned and added to list.\n",
      "Dataset 15 cleaned and added to list.\n",
      "Dataset 16 cleaned and added to list.\n",
      "Dataset 17 cleaned and added to list.\n",
      "Dataset 18 cleaned and added to list.\n",
      "Dataset 19 cleaned and added to list.\n",
      "Dataset 20 cleaned and added to list.\n",
      "Dataset 21 cleaned and added to list.\n",
      "Dataset 22 cleaned and added to list.\n",
      "Dataset 23 cleaned and added to list.\n",
      "Dataset 24 cleaned and added to list.\n",
      "Dataset 25 cleaned and added to list.\n",
      "Dataset 26 cleaned and added to list.\n",
      "Dataset 27 cleaned and added to list.\n",
      "Dataset 28 cleaned and added to list.\n",
      "Dataset 29 cleaned and added to list.\n",
      "Dataset 30 cleaned and added to list.\n",
      "Dataset 31 cleaned and added to list.\n",
      "Dataset 32 cleaned and added to list.\n",
      "Dataset 33 cleaned and added to list.\n",
      "Dataset 34 cleaned and added to list.\n",
      "Dataset 35 cleaned and added to list.\n",
      "Dataset 36 cleaned and added to list.\n",
      "Dataset 37 cleaned and added to list.\n",
      "Dataset 38 cleaned and added to list.\n",
      "Dataset 39 cleaned and added to list.\n",
      "Dataset 40 cleaned and added to list.\n",
      "Dataset 41 cleaned and added to list.\n",
      "Dataset 42 cleaned and added to list.\n",
      "Dataset 43 cleaned and added to list.\n",
      "Dataset 44 cleaned and added to list.\n",
      "Dataset 45 cleaned and added to list.\n",
      "Dataset 46 cleaned and added to list.\n",
      "Dataset 47 cleaned and added to list.\n",
      "Dataset 48 cleaned and added to list.\n",
      "Dataset 49 cleaned and added to list.\n",
      "Dataset 50 cleaned and added to list.\n",
      "Dataset 51 cleaned and added to list.\n"
     ]
    }
   ],
   "source": [
    "#STEP 31: Call the function\n",
    "csvs_data_file = 'C:\\\\Users\\\\ssteffen\\\\University of Idaho\\\\Storage-Boise - ECHO\\\\Staff\\\\Sam\\\\Data\\\\Projects\\\\MailChimp_formatter\\\\raw_MailChimp\\\\input'\n",
    "\n",
    "clean_csvs(csvs_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAST STEP: Delete all the downloaded raw .csv files from the raw_MailChimp/input folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
